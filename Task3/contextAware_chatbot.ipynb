{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5e7d76",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e2ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: cement.txt\n",
      "Loaded: ml.txt\n",
      "Loaded: pakistan_history.txt\n",
      "Total documents loaded: 3\n",
      "Total chunks created: 6\n",
      "Vector store saved to faiss_store\n",
      "Vector store saved successfully!\n",
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# Alternative: Use TfidfVectorizer for embeddings (no download needed)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def build_vector_store():\n",
    "    docs = []\n",
    "    folder_path = \"docs/\"\n",
    "\n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: '{folder_path}' directory not found!\")\n",
    "        return\n",
    "\n",
    "    # Load all text files\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(folder_path, file), encoding='utf-8')\n",
    "                docs.extend(loader.load())\n",
    "                print(f\"Loaded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not docs:\n",
    "        print(\"No documents found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total documents loaded: {len(docs)}\")\n",
    "\n",
    "    # Split documents into chunks\n",
    "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    print(f\"Total chunks created: {len(split_docs)}\")\n",
    "\n",
    "    # Create a simple embedding function using TfidfVectorizer\n",
    "    class LocalEmbeddings:\n",
    "        def __init__(self):\n",
    "            self.vectorizer = TfidfVectorizer(max_features=100)\n",
    "            \n",
    "        def embed_documents(self, texts):\n",
    "            return self.vectorizer.fit_transform(texts).toarray().astype(np.float32)\n",
    "        \n",
    "        def embed_query(self, text):\n",
    "            return self.vectorizer.transform([text]).toarray()[0].astype(np.float32)\n",
    "    \n",
    "    embeddings = LocalEmbeddings()\n",
    "    \n",
    "    # Create embeddings for all documents\n",
    "    doc_texts = [doc.page_content for doc in split_docs]\n",
    "    embeddings_array = embeddings.embed_documents(doc_texts)\n",
    "    \n",
    "    # Build FAISS vector store\n",
    "    import faiss\n",
    "    dimension = embeddings_array.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_array)\n",
    "    \n",
    "    # Create a wrapper for FAISS that works with our embeddings\n",
    "    class FAISSWrapper:\n",
    "        def __init__(self, index, embeddings, docs):\n",
    "            self.index = index\n",
    "            self.embeddings = embeddings\n",
    "            self.docs = docs\n",
    "        \n",
    "        def save_local(self, path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            faiss.write_index(self.index, f\"{path}/index.faiss\")\n",
    "            import pickle\n",
    "            with open(f\"{path}/docs.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.docs, f)\n",
    "            print(f\"Vector store saved to {path}\")\n",
    "    \n",
    "    vector_store = FAISSWrapper(index, embeddings, split_docs)\n",
    "    vector_store.save_local(\"faiss_store\")\n",
    "    print(\"Vector store saved successfully!\")\n",
    "\n",
    "build_vector_store()\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e1d23",
   "metadata": {},
   "source": [
    "### Create Chatbot (with memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87925e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized successfully!\n",
      "Loaded 6 documents from vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asfa\\AppData\\Local\\Temp\\ipykernel_15896\\1912391973.py:49: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def create_chatbot():\n",
    "    # Load the saved FAISS index\n",
    "    faiss_index = faiss.read_index(\"faiss_store/index.faiss\")\n",
    "    \n",
    "    # Load the documents and embeddings\n",
    "    with open(\"faiss_store/docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "    \n",
    "    # Recreate the embeddings object\n",
    "    class LocalEmbeddings:\n",
    "        def __init__(self):\n",
    "            self.vectorizer = TfidfVectorizer(max_features=100)\n",
    "            # Fit on the existing documents\n",
    "            doc_texts = [doc.page_content for doc in docs]\n",
    "            self.vectorizer.fit(doc_texts)\n",
    "            \n",
    "        def embed_documents(self, texts):\n",
    "            return self.vectorizer.transform(texts).toarray().astype(np.float32)\n",
    "        \n",
    "        def embed_query(self, text):\n",
    "            return self.vectorizer.transform([text]).toarray()[0].astype(np.float32)\n",
    "        \n",
    "        def similarity_search(self, query, k=4):\n",
    "            query_embedding = self.embed_query(query)\n",
    "            query_embedding = np.array([query_embedding])\n",
    "            distances, indices = faiss_index.search(query_embedding, k)\n",
    "            return [docs[i] for i in indices[0]]\n",
    "    \n",
    "    embeddings = LocalEmbeddings()\n",
    "    \n",
    "    # Create a simple retriever\n",
    "    class SimpleRetriever:\n",
    "        def __init__(self, embeddings, docs):\n",
    "            self.embeddings = embeddings\n",
    "            self.docs = docs\n",
    "            \n",
    "        def get_relevant_documents(self, query):\n",
    "            return self.embeddings.similarity_search(query)\n",
    "    \n",
    "    retriever = SimpleRetriever(embeddings, docs)\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    \n",
    "    # For now, create a simple chatbot without LlamaCpp (since model file may not exist)\n",
    "    # You can replace this with actual LLM later\n",
    "    print(\"Chatbot initialized successfully!\")\n",
    "    print(f\"Loaded {len(docs)} documents from vector store\")\n",
    "    \n",
    "    return retriever, memory\n",
    "\n",
    "retriever, memory = create_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f3a5a",
   "metadata": {},
   "source": [
    "### Test Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b0b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is cement and how is it made?\n",
      "\n",
      "==================================================\n",
      "Relevant Documents:\n",
      "\n",
      "Document 1:\n",
      "Cement is a fine powder made from limestone and other minerals, which acts as a binder when mixed with water. The main ingredient of cement is calcium carbonate, usually derived from limestone. Cement is used in construction as a binding material to hold together bricks, stones, and concrete.\n",
      "\n",
      "The production process involves mining limestone, crushing it, and heating it in a kiln at high temperatures. This process produces clinker, which is then ground into a fine powder and mixed with gypsum. C\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "In construction, cement is mixed with sand and aggregate to form concrete. Concrete is one of the most widely used building materials due to its strength, durability, and flexibility in construction design. The ratio of cement to sand and aggregate determines the strength of the concrete. The water-cement ratio is also important; too much water can weaken the concrete.\n",
      "\n",
      "Cement has a major role in modern construction, used in buildings, bridges, roads, dams, and many other structures. It is an es\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Pakistan is a diverse country with many languages, cultures, and traditions. The national language is Urdu, while English is also widely used in government and education. The countryâ€™s economy is based on agriculture, manufacturing, and services. Major industries include textiles, agriculture, and mining.\n",
      "\n",
      "Pakistan has faced political and economic challenges but continues to develop and grow. The country has a strong sense of national identity and cultural heritage.\n",
      "--------------------------------------------------\n",
      "Document 4:\n",
      "Machine Learning (ML) is a branch of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms to identify patterns and make decisions based on data. ML is divided into several types:\n",
      "\n",
      "1. Supervised Learning: The model learns from labeled data. Example algorithms include Linear Regression, Logistic Regression, Decision Trees, and Support Vector Machines.\n",
      "\n",
      "2. Unsupervised Learning: The model learns from unlabeled data. It identifies \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever with a query\n",
    "query = \"What is cement and how is it made?\"\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Relevant Documents:\\n\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(doc.page_content[:500])  # Print first 500 chars\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
